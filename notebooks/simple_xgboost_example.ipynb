{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton server with FIL backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will run through the procedure to deploy a XGBoost model in Triton Inference Server with Forest Inference Library (FIL) backend. The FIL backend allows forest models trained by several popular machine learning frameworks (including XGBoost, LightGBM, Scikit-Learn, and cuML) to be deployed in a Triton inference server using the RAPIDS Forest Inference LIbrary for fast GPU-based inference. Using this backend, forest models can be deployed seamlessly alongside deep learning models for fast, unified inference pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* Nvidia GPU (T4 or V100 or A100)\n",
    "* [Latest NVIDIA driver](https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html)\n",
    "* [Docker](https://docs.docker.com/get-docker/)\n",
    "* [The NVIDIA container toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running this notebook, please check whether NVIDIA driver, Docker and NVIDIA CUDA Toolkit. `nvidia-smi` command should run successfully as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install XGBoost and Sklearn\n",
    "\n",
    "We'd need to install XGBoost and SKlearn using the following pip3 commands inside the container as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sklearn first\n",
    "!pip3 install -U scikit-learn\n",
    "\n",
    "# Then install XGBoost\n",
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model\n",
    "\n",
    "If you have a pre-trained xgboost model, save it as `xgboost.model` and skip this step. We'll train a XGBoost model on random data in this section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import signal\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data to perform binary classification\n",
    "seed = 7\n",
    "features = 9 # number of sample features\n",
    "samples = 10000 # number of samples\n",
    "X = numpy.random.rand(samples, features).astype('float32')\n",
    "Y = numpy.random.randint(2, size=samples)\n",
    "\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and load XGBoost model in Triton Inference Server\n",
    "\n",
    "For deploying the trained XGBoost model in Triton Inference Server, follow the steps below:\n",
    "\n",
    "**1. Create a model repository and save xgboost model checkpoint:**\n",
    "\n",
    "We'll need to create a model repository that looks as follows:\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "`-- fil\n",
    "    |-- 1\n",
    "    |   `-- xgboost.model\n",
    "    `-- config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to save the model\n",
    "![ ! -d \"/data/model_repository\" ] && mkdir -p /data/model_repository/fil/1\n",
    "\n",
    "# Save your xgboost model as xgboost.model\n",
    "model.save_model('/data/model_repository/fil/1/xgboost.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create and save config.pbtxt**\n",
    "\n",
    "To deploy the model in Triton Inference Server, we need to create and save a protobuf config file called config.pbtxt under `model_repository/fil/` directory that contains information about the model and the deployment. Sample config file is available here: [link](https://github.com/triton-inference-server/fil_backend#configuration)\n",
    "\n",
    "Essentially, the following parameters need to be updated as per your configuration\n",
    "\n",
    "```\n",
    "name: \"fil\"                              # Name of the model directory (fil in our case)\n",
    "backend: \"fil\"                           # Triton FIL backend for deploying forest models\n",
    "max_batch_size: 8192\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 9 ]                          # Input feature dimensions, in our sample case it's 9\n",
    "  }\n",
    "]\n",
    "output [\n",
    " {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]                          # Output 2 for binary classification model\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_GPU }]\n",
    "parameters [\n",
    "  {\n",
    "    key: \"model_type\"\n",
    "    value: { string_value: \"xgboost\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"predict_proba\"\n",
    "    value: { string_value: \"false\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"output_class\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"threshold\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"algo\"\n",
    "    value: { string_value: \"ALGO_AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"storage_type\"\n",
    "    value: { string_value: \"AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"blocks_per_sm\"\n",
    "    value: { string_value: \"0\" }\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [1024, 8192]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n",
    "Store the above config at `/data/model_repository/fil/` directory as config.pbtxt as follows:\n",
    "\n",
    "For more information on sample configs, please refer this [link](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Writing config to file\n",
    "cat > /data/model_repository/fil/config.pbtxt <<EOL \n",
    "name: \"fil\"                              # Name of the model directory (fil in our case)\n",
    "backend: \"fil\"                           # Triton FIL backend for deploying forest models\n",
    "max_batch_size: 8192\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 9 ]                          # Input feature dimensions, in our sample case it's 9\n",
    "  }\n",
    "]\n",
    "output [\n",
    " {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]                          # Output 2 for binary classification model\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_GPU }]\n",
    "parameters [\n",
    "  {\n",
    "    key: \"model_type\"\n",
    "    value: { string_value: \"xgboost\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"predict_proba\"\n",
    "    value: { string_value: \"false\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"output_class\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"threshold\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"algo\"\n",
    "    value: { string_value: \"ALGO_AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"storage_type\"\n",
    "    value: { string_value: \"AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"blocks_per_sm\"\n",
    "    value: { string_value: \"0\" }\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [1024, 8192]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the model repository should look like this:\n",
    "\n",
    "```\n",
    "model_repository/\n",
    "`-- fil\n",
    "    |-- 1\n",
    "    |   `-- xgboost.model\n",
    "    `-- config.pbtxt\n",
    "```\n",
    "\n",
    "**3. Deploy the model in Triton Inference Server**\n",
    "\n",
    "Finally, we can deploy the xgboost model in Triton Inference Server using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Triton Inference Server in a Subprocess from Jupyter notebook\n",
    "\n",
    "# The os.setsid() is passed in the argument preexec_fn so\n",
    "# it's run after the fork() and before  exec() to run the shell.\n",
    "pro = subprocess.Popen([\"tritonserver --model-repository=/data/model_repository\"], stdout=subprocess.PIPE, \n",
    "                       shell=True, preexec_fn=os.setsid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command should load the model and print the log `successfully loaded 'fil' version 1`. Triton server listens on the following endpoints:\n",
    "\n",
    "```\n",
    "Port 8000    -> HTTP Service\n",
    "Port 8001    -> GRPC Service\n",
    "Port 8002    -> Metrics\n",
    "```\n",
    "\n",
    "We can test the status of the server connection by running the curl command: `curl -v <IP of machine>:8000/v2/health/ready` which should return `HTTP/1.1 200 OK`\n",
    "\n",
    "**NOTE:-** In our case the IP of machine on which Triton Server and this notebook are currently running is `localhost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Triton Server before proceeding further\n",
    "os.killpg(os.getpgid(pro.pid), signal.SIGTERM)  # Send the signal to all the process groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent Model Execution\n",
    "\n",
    "By default the Triton Inference Server loads a single instance of our XGBoost model. We can load multiple instances of the model on either the same GPU or different GPU (depending on the availability) inorder to improve the throughput of incoming requests. For more information on concurrent model execution in Triton, please refer [link](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#instance-groups) and [link](https://github.com/triton-inference-server/server/blob/r21.06/docs/architecture.md#concurrent-model-execution)\n",
    "\n",
    "To deploy multiple instances of our XGBoost model, we need to make a small modification in instance_group within config.pbtxt file. Change `instance_group [{ kind: KIND_GPU }]` to\n",
    "\n",
    "```\n",
    "  instance_group [\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "This will result in Triton server creating 2 instances of XGBoost model on the same GPU. Run the server after making the above modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Writing config to file\n",
    "cat > /data/model_repository/fil/config.pbtxt <<EOL \n",
    "name: \"fil\"                              # Name of the model directory (fil in our case)\n",
    "backend: \"fil\"                           # Triton FIL backend for deploying forest models\n",
    "max_batch_size: 8192\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 9 ]                          # Input feature dimensions, in our sample case it's 9\n",
    "  }\n",
    "]\n",
    "output [\n",
    " {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 2 ]                          # Output 2 for binary classification model\n",
    "  }\n",
    "]\n",
    "instance_group [                         # 2 instances of the model will be deployed in this case\n",
    "    {\n",
    "      count: 2\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "parameters [\n",
    "  {\n",
    "    key: \"model_type\"\n",
    "    value: { string_value: \"xgboost\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"predict_proba\"\n",
    "    value: { string_value: \"false\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"output_class\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"threshold\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"algo\"\n",
    "    value: { string_value: \"ALGO_AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"storage_type\"\n",
    "    value: { string_value: \"AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"blocks_per_sm\"\n",
    "    value: { string_value: \"0\" }\n",
    "  }\n",
    "]\n",
    "\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [1024, 8192]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Triton Inference Server with 2 instances of our model in a Subprocess from Jupyter notebook\n",
    "\n",
    "# The os.setsid() is passed in the argument preexec_fn so\n",
    "# it's run after the fork() and before  exec() to run the shell.\n",
    "pro = subprocess.Popen([\"tritonserver --model-repository=/data/model_repository\"], stdout=subprocess.PIPE, \n",
    "                       shell=True, preexec_fn=os.setsid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above logs, two instances of XGBoost model are created using Triton FIL backend\n",
    "\n",
    "```\n",
    "I0716 17:55:38.954195 313 api.cu:117] TRITONBACKEND_ModelInstanceInitialize: fil_0_0 (GPU device 0)\n",
    "I0716 17:55:43.991152 313 api.cu:117] TRITONBACKEND_ModelInstanceInitialize: fil_0_1 (GPU device 0)\n",
    "I0716 17:55:44.000355 313 model_repository_manager.cc:1212] successfully loaded 'fil' version 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Triton Server before proceeding further\n",
    "os.killpg(os.getpgid(pro.pid), signal.SIGTERM)  # Send the signal to all the process groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Batching\n",
    "\n",
    "Dynamic batching is a feature of Triton that allows inference requests to be combined by the server, so that a batch is created dynamically. This results in increased throughput. To perform dynamic batching in Triton you'd need to modify the following property in config.pbtxt file:\n",
    "```\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [1024, 8192]\n",
    "  max_queue_delay_microseconds: 100\n",
    "}\n",
    "```\n",
    "The *preferred_batch_size* property indicates the batch sizes that the dynamic batcher should attempt to create. The *max_queue_delay_microseconds* property setting delays sending the batch, when a batch of a preferred size cannot be created from the available requests as long as no request is delayed longer than the configured max_queue_delay_microseconds value. If a new request arrives during this delay and allows the dynamic batcher to form a batch of a preferred batch size, then that batch is sent immediately for inferencing. If the delay expires the dynamic batcher sends the batch as is, even though it is not a preferred size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analyzer\n",
    "\n",
    "[Triton Model Analyzer](https://github.com/triton-inference-server/model_analyzer) is a tool to profile and evaluate the best deployment configuration that maximizes inference performance of your model when deployed in Triton Inference Server. Using this tool, you can find the appropriate batch size, #TODO benefits of model analyzer. Model Analyzer installation steps are available here: [link](https://github.com/triton-inference-server/model_analyzer/blob/main/docs/install.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nvidia-pyindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install triton-model-analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export DCGM_VERSION=2.0.13\n",
    "!wget -q https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/datacenter-gpu-manager_${DCGM_VERSION}_amd64.deb && \\\n",
    " dpkg -i datacenter-gpu-manager_${DCGM_VERSION}_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!model-analyzer profile -m /data/model_repository/ --profile-models fil --override-output-model-repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton Client\n",
    "\n",
    "**Note:-** Triton server needs to be running for executing this section\n",
    "\n",
    "After model profiling is done and the final model is selected as per required configuration and deployed in Triton, we can now test the inference by sending inference request from Triton Client. For more information on installation steps, please check [Triton Client Github](https://github.com/triton-inference-server/client)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Triton Inference Server in background from Jupyter notebook\n",
    "\n",
    "# The os.setsid() is passed in the argument preexec_fn so\n",
    "# it's run after the fork() and before  exec() to run the shell.\n",
    "pro = subprocess.Popen([\"tritonserver --model-repository=/data/model_repository\"], stdout=subprocess.PIPE, \n",
    "                       shell=True, preexec_fn=os.setsid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies to run Triton Client from Triton Inference Server container\n",
    "!pip3 install nvidia-pyindex\n",
    "!pip3 install tritonclient[http] # Install just http client for this notebook demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check client library can be imported\n",
    "import numpy\n",
    "import tritonclient.http as triton_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up HTTP client.\n",
    "http_client = triton_http.InferenceServerClient(\n",
    "    url='localhost:8000',\n",
    "    verbose=False,\n",
    "    concurrency=1\n",
    ")\n",
    "\n",
    "# Set up Triton input and output objects for both HTTP and GRPC\n",
    "triton_input_http = triton_http.InferInput(\n",
    "    'input__0',\n",
    "    (X_test.shape[0], X_test.shape[1]),\n",
    "    'FP32'\n",
    ")\n",
    "triton_input_http.set_data_from_numpy(X_test, binary_data=True)\n",
    "triton_output_http = triton_http.InferRequestedOutput(\n",
    "    'output__0',\n",
    "    binary_data=True\n",
    ")\n",
    "\n",
    "# Submit inference requests \n",
    "request_http = http_client.infer(\n",
    "    'fil',\n",
    "    model_version='1',\n",
    "    inputs=[triton_input_http],\n",
    "    outputs=[triton_output_http]\n",
    ")\n",
    "\n",
    "# Get results as numpy arrays\n",
    "result_http = request_http.as_numpy('output__0')\n",
    "\n",
    "# Check that we got the same accuracy as previously\n",
    "predictions = [round(value) for value in result_http]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test accuracy score of the model deployed in Triton using FIL backend matches with the one previously computed using XGBoost library's predict function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
